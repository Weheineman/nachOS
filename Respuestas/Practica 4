Ejercicio 1:
Cambiamos las funciones de userprog/transfer.cc ya que ahora Machine::ReadMem puede devolver false en caso de que la dirección buscada no esté en la TLB. Entonces, modificamos la lectura y escritura de dichas funciones para que intente tres veces antes de darse por vencida y falle. Elegimos un número arbitrario de intentos que intente asegurar que una lectura legal pueda realizarse, mientras que las erróneas fallen de todas formas.

Ejercicio 2:
Tomamos el hit ratio como la siguiente fórmula: successfulReads / totalReads
Donde: - successfulReads es el número de lecturas a memoria exitosas (Hits).
       - totalReads es el número de lecturas a memoria (Hits + Misses).

En este caso, consideramos que una lectura fallida en la TLB seguida por un segundo intento exitoso sólo cuenta como una única lectura fallida. Es decir, en ese caso se tendría solamente un miss, no un hit y un miss.
Como en nuestro código tenemos disponibles todas las lecturas en memoria (numMemoryReads) y la cantidad de fallas de página (pageFaults), y cada pageFault implica una lectura, tenemos que eliminar las lecturas repetidas que nos implica tener un miss seguido por el hit subsiguiente, pues queremos que eso cuente como una única lectura fallida y no como una exitosa y una fallida.
Por lo tanto, la fórmula final sería: successfulReads / totalReads = (totalMemoryReads - pageFaults) / totalMemoryReads = ((numMemoryReads - pageFaults) - pageFaults) / (numMemoryReads - pageFaults)

Luego, los valores obtenidos al ejecutar los programas matmult_halt y sort_halt son:
matmult_halt Hit Ratio: TLB de 4 páginas = 88.6001
                        TLB de 32 páginas = 99.9818
                        TLB de 64 páginas = 99.9909
sort_halt Hit Ratio: TLB de 4 páginas = 93.8467
                     TLB de 32 páginas = 99.9885
                     TLB de 64 páginas = 99.9999

Es importante notar que los resultados no son demasiado distantes entre sí, al pasar de 32 a 64 páginas. La diferencia es apenas del 0.01%.
Además, aumentar la cantidad de páginas reduce notoriamente la cantidad de pageFaults obtenidos:
matmult_halt pageFaults: TLB de 4 páginas = 83381
                        TLB de 32 páginas = 126
                        TLB de 64 páginas = 63
sort_halt pageFaults: TLB de 4 páginas = 2639153
                      TLB de 32 páginas = 4883
                      TLB de 64 páginas = 56

Si bien a primera vista parece que aumentar el tamaño de TLB es lo más deseable, esto provocaría que el sistema se ralentice. Cada vez que ocurre un cambio de contexto, las entradas de la tlb se anulan y vuelven a escribir, por lo que a mayor tamaño, mayor tiempo se pierde en el proceso. En el caso de busqueda infructuosa, el tiempo que se pierde entre una TLB de tamaño 32 y una TLB de tamaño 64 es del doble.
Visto y considerando que la diferencia en Hit Ratio y pageFault en una TLB de 32 a 64 páginas no es relevante, lo óptimo sería escoger un tamaño de 32 páginas.
En general, consideramos que la mejor estrategia es optar por un tamaño intermedio. Lo suficientemente grande como para obtener un buen Hit Ratio y una baja cantidad de pageFaults, sin sacrificar mucho tiempo de cómputo. En particular, creemos que 16 páginas es un buen valor de referencia.

Ejercicio 3:
Para utilizar Demand Loading, tiene que utilizarse la bandera DEMAND_LOADING, agregándola en el Makefile del directorio vmem.

Ejercicio 4:
Para preservar el codigo de ejercicios anteriores, en AddressSpace::LoadPage() physIndex vale un valor dado por el coreMap (implementacion nueva, ej4) o toma el valor antiguo (la pagina física reservada en el momento de creación, ej1).

Ejercicio 5:
Se agregó una nueva flag de compilación, LRU. De estar en el Makefile, Nachos utiliza LRU para elegir qué página física enviar a la swap. De no estar definida, utiliza FIFO.
Para probar su efectividad, se contaron la cantidad de veces que una página se carga desde la swap a la memoria. Se agregó una nueva bandera de debugging para esto: 'w' (en honor al sistema operativo privativo favorito de Mariano Street).
Se corrieron los comandos desde code/vmem: ./nachos -d w -x ../userland/matmult_halt
                                           ./nachos -d w -x ../userland/sort_halt
Cada uno se corrió dos veces, una vez compilando nachos con la bandera LRU y la otra sin.

Con LRU:
    matmult_halt:   5513 loads de swap a memoria
    sort_halt:     16847 loads de swap a memoria

Con FIFO:
    matmult_halt:  23240 loads de swap a memoria
    sort_halt:     30338 loads de swap a memoria
